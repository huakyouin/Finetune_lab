{
    "base": "qwen",
    "base_id": "base/qwen/Qwen2-0_5B-Instruct",

    "lora_config": {
        "task_type": "CAUSAL_LM",
        "r": 8,
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "lora_alpha": 16,
        "use_rslora": true,
        "lora_dropout": 0.1,
        "bias": "none",
        "inference_mode": false
    },
    "lora_save_path": "outputs/Lora/Qwen2-0_5B-instruct-lora",

    "max_len": 384,
    "sample_size": 2e4,
    "dataset_settings": [
        {
            "path": "data/findata",
            "weight": 1,
            "filter_cols": ["FINNA"]
        }
    ],

    
    "training_args": {
        "per_device_train_batch_size": 2,
        "gradient_accumulation_steps": 4,
        "learning_rate": 2e-5,
        "fp16": true,
        "logging_steps": 10,
        "optim": "adamw_torch",
        "weight_decay": 0.01,
        "output_dir": "outputs/qwen2_ckpts",
        "num_train_epochs": 1,
        "seed": 3047,
        "save_steps": 200,
        "eval_steps": 500,
        "gradient_checkpointing": true,
        "save_total_limit": 30
    }
    
}