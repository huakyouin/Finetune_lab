{
    
    "model_id": "models/Base/Qwen-7B-Chat",
    "lora_config": {
        "task_type": "CAUSAL_LM",
        "r": 16,
        "target_modules": ["c_attn", "c_proj", "w1", "w2"],
        "lora_alpha": 16,
        "use_rslora": true,
        "lora_dropout": 0.05,
        "bias": "none",
        "inference_mode": false
    },
    


    "MAX_LENGTH": 384,
    "sample_size": 1e4,
    "dataset_settings": [
        {
            "path": "datasets/alpaca-data-gpt4-chinese",
            "weight": 0.1,
            "rename_mapping": {
                "instruction_zh": "instruction",
                "input_zh": "input",
                "output_zh": "output"
            }
        }
    ],

    
    "training_args": {
        "per_device_train_batch_size": 2,
        "gradient_accumulation_steps": 4,
        "learning_rate": 5e-5,
        "fp16": true,
        "logging_steps": 10,
        "optim": "adamw_torch",
        "weight_decay": 0.01,
        "output_dir": "outputs/qwen_step1",
        "num_train_epochs": 1,
        "seed": 3047,
        "save_steps": 200,
        "eval_steps": 500,
        "gradient_checkpointing": true,
        "save_total_limit": 30
    },

    "new_lora_path": "models/Lora/Qwen_7B_Chat_lora_step1"
}