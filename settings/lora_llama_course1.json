{
    "base": "llama",
    "model_id": "models/Base/Llama3-Chinese-8B-Instruct",
    "new_lora_path": "models/llama3_chinese_8b_lora_step1",

    "lora_config": {
        "task_type": "CAUSAL_LM",
        "r": 8,
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "lora_alpha": 16,
        "use_rslora": true,
        "lora_dropout": 0.1,
        "bias": "none",
        "inference_mode": false
    },

    "max_len": 384,
    "sample_size": 1e4,
    "dataset_settings": [
        {
            "path": "datasets/alpaca-data-gpt4-chinese",
            "weight": 0.1,
            "rename_mapping": {
                "instruction_zh": "instruction",
                "input_zh": "input",
                "output_zh": "output"
            }
        }
    ],

    
    "training_args": {
        "per_device_train_batch_size": 2,
        "gradient_accumulation_steps": 4,
        "learning_rate": 1e-4,
        "fp16": true,
        "logging_steps": 10,
        "optim": "adamw_torch",
        "weight_decay": 0.01,
        "output_dir": "outputs/llama3_step1",
        "num_train_epochs": 1,
        "seed": 3047,
        "save_steps": 200,
        "eval_steps": 500,
        "gradient_checkpointing": true,
        "save_total_limit": 30
    }
}