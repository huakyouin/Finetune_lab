{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/newdisk/jxh/anaconda/envs/sft2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import transformers\n",
    "from typing import Dict\n",
    "from collections import defaultdict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq, EvalPrediction\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, load_from_disk\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel,PeftConfig\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers.trainer_pt_utils import LabelSmoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear8bitLt(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear8bitLt(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear8bitLt(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id= \"base/qwen/Qwen2-0_5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16, load_in_8bit=True, trust_remote_code=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_disk(\"outputs/final/Qwen2-0_5B-instruct-lora/eval_data\")\n",
    "# data = concatenate_datasets([d for key, d in data.items() if isinstance(d, Dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "def preprocess(\n",
    "    messages,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_len: int,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocesses the data for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    texts = []\n",
    "    for i, msg in enumerate(messages):\n",
    "        texts.append(\n",
    "            tokenizer.apply_chat_template(\n",
    "                msg,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=False,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_len,\n",
    "                truncation=True,\n",
    "            )\n",
    "        )\n",
    "    input_ids = torch.tensor(texts, dtype=torch.int)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[target_ids == tokenizer.pad_token_id] = IGNORE_TOKEN_ID\n",
    "    print(tokenizer.decode(target_ids))\n",
    "    attention_mask = input_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": target_ids\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"}, {\"role\": \"assistant\", \"content\": \"Large language models are a type of language model that is trained on a large corpus of text data. They are capable of generating human-like text and are used in a variety of natural language processing tasks...\"}]]\n",
    "d = preprocess(messages,tokenizer,384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "d = preprocess(data[\"message\"],tokenizer,584)\n",
    "# Evaluate on test set\n",
    "predictions = []\n",
    "references = data[\"output\"]\n",
    "\n",
    "output = model.generate(d.to(model.device), max_length=584, num_beams=4, early_stopping=True)\n",
    "prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "predictions.append(prediction)\n",
    "\n",
    "# Calculate BLEU and ROUGE\n",
    "rouge = load_metric(\"rouge\")\n",
    "bleu = load_metric(\"sacrebleu\")\n",
    "\n",
    "result_rouge = rouge.compute(predictions=predictions, references=references)\n",
    "result_bleu = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "\n",
    "print(\"ROUGE:\", result_rouge)\n",
    "print(\"BLEU:\", result_bleu[\"score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sft2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
